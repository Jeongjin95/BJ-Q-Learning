num_states <- 2
num_actions <- 3
Q_list <- vector("list", n)
for (j in 1:n) {
  Q_list[[j]] <- matrix(0, nrow = num_states, ncol = num_actions)
}
alpha <- 0.1  # Learning rate
gamma <- 0.9  # Discount factor
epsilon <- 0.2  # Epsilon for epsilon-greedy policy
num_iterations <- 1000  # Number of iterations for Q-value updates

for (j in 1:n) {
  for (iteration in 1:num_iterations) {
    # Update Q-value for state 2
    state2 <- 2
    if (runif(1) < epsilon) {
      # Explore: choose a random action
      action2 <- sample(1:num_actions, 1)
    } else {
      # Exploit: choose the action with the highest Q-value
      action2 <- which.max(Q_list[[j]][state2, ])
    }
    survival2 <- Imputed.Surv.2[j]
    reward2 <- c(survival2, survival2, survival2)[action2] + 
      5 * c(Severity2[j] == 3, Severity2[j] == 2, Severity2[j] == 1)[action2]
    Q_list[[j]][state2, action2] <- (1 - alpha) * Q_list[[j]][state2, action2] + alpha * reward2
    
    # Update Q-value for state 1
    state1 <- 1
    if (runif(1) < epsilon) {
      # Explore: choose a random action
      action1 <- sample(1:num_actions, 1)
    } else {
      # Exploit: choose the action with the highest Q-value
      action1 <- which.max(Q_list[[j]][state1, ])
    }
    survival1 <- Surv1[j]
    reward1 <- c(survival1, survival1, survival1)[action1] +
      5 * c(Severity1[j] == 3, Severity1[j] == 2, Severity1[j] == 1)[action1]
    Q_list[[j]][state1, action1] <- (1 - alpha) * Q_list[[j]][state1, action1] + alpha * (reward1 + gamma * max(Q_list[[j]][state2, ]))
  }
}
